НАЧАЛЬНЫЕ ИЗМЕНЕНИЯ НЕПОСРЕДСТВЕННО ПОСЛЕ УСТАНОВКИ:
## в  ~/new5/baselines/baselines/run.py    добавить import pybulletgym
## туда же добавить           env.render()    перед    "model = learn(    <...>"
## в  runner.py   добавить   env.render()    после    "self.obs[:], rewards, self.dones, infos = self.env.step(actions)"

ОСТАЛЬНОЕ:
Файлы типа ant.xml  лежат в   /home/ai/new5/pybullet-gym/pybulletgym/envs/assets/mjcf/ 
env.step() определено в   /home/ai/new5/pybullet-gym/pybulletgym/envs/roboschool/envs/locomotion/walker_base_env.py

Детерминизм получается, если в walker_base.py убрать random в robot_specific_reset  (сам pybullet claims to be 100% deterministic)
Многоагентность получается вставкой куска кода в runner.py

РЕГИСТРАЦИЯ НОВОГО ENVIRONMENT
## взято из https://stable-baselines.readthedocs.io/en/master/guide/custom_env.html
## а также из https://stackoverflow.com/questions/52727233/how-can-i-register-a-custom-environment-in-openais-gym
(1) положить CustomEnv.py в  pybulletgym/envs/roboschool/robots/locomotors/
(2) добавить в __init__.py в той же папке строчку:
from pybulletgym.envs.roboschool.robots.locomotors.CustomEnv import CustomEnv
(3) внутри pybulletgym/envs/__init__.py зарегистрировать свой енвайронмент строчками:
register(
	id='CustomEnv-v0',
	entry_point='pybulletgym.envs.roboschool.robots.locomotors.CustomEnv:CustomEnv',
	max_episode_steps=1000,
	reward_threshold=2500.0
	)

ИСПРАВЛЕНИЕ ОШИБКИ ДЛЯ МУЛЬТИАГЕНТНОСТИ (2019-11-20--13-50)
Раньше в ppo2/runner.py просто менялось action = muag_change_actions(...)
    но это приводило к ошибке -- в мультиагентном сценарии агент номер 1 не мог даже учиться стоять в в заданной точке
    (в то время как тот же запуск с agent_number==0 обучался этому в пределах 2-3 минут)
Исправлено на action_for_sim = muag_change_actions(...)
    причем actions_for_sim используется только для подачи на вход симулятора env.step()
Предположительный источник ошибки и почему она исправлена:
    поскольку в прошлом варианте нейросеть обучалась на действиях обоих агентов,
    то это могло приводить к возникновению нежеланных корреляций и смещенностей
    (хотя действия 2го агента и не вытекали явно из вычислений нейросетки 1го агента, но неявно корреляции могли быть)
    А в новом варианте вторая часть действий 1го агента просто абсолютно никак ни на что не влияет и не скоррелирована ни с чем
    следовательно и корреляций и нежеланных смещенностей быть не должно
Что и иллюстрируется в коммите на примере задачки стремления агентов каждого к своей точке.

